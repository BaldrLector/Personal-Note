#

## Basic Theory

### 卷积的作用

卷积是一种局部操作，通过一定大小的卷积核作用于局部图像区域获得图像的局部信息。试想，若原图像素`(x, y)`处可能存在物体边缘，则其四周`(x−1, y)`，`(x+1, y)`，`(x, y − 1)`，`(x, y + 1)`处像素值应与`(x, y)`处有显著差异。此时，如作用以整体边缘滤波器${K_e}$，可消除四周像素值差异小的图像区域而保留显著差异区域，以此可检测出物体边缘信息。同理，类似${K_h}$ 和${K_V^3}$的横向、纵向边缘滤波器可分别保留横向、纵向的边缘信息

事实上，卷积网络中的卷积核参数是通过网络训练学出的，除了可以学到类似的横向、纵向边缘滤波器，还可以学到任意角度的边缘滤波器。当然，不仅如此，检测颜色、形状、纹理等等众多基本模式的滤波器（卷积核）都可以包含在一个足够复杂的深层卷积神经网络中。通过“组合”这些滤波器（卷积核）以及随着网络后续操作的进行，基本而一般的模式会逐渐被抽象为具有高层语义的“概念”表示，并以此对应到具体的样本类别。颇有“盲人摸象”后，将各自结果集大成之意。

### 池化层
Max、Average、Stochastic pooling<br>
`池化`是一种常见的降采样方法，也可以看成一个用p-范数作为非线性映射的`卷积`<br>
研究者普遍认为池化层有以下作业：
- 特征不变性<br>
汇合操作使模型更关注是否存在某些特征而不是特征具体的位置。可看作是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。
- 特征降维<br>
由于汇合操作的降采样作用，汇合结果中的一个元素对应于原
输入数据的一个子区域，因此汇合相当于在空间范围内做了
维度约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。
- 一定程度防止过拟合

### 激活函数

目的：增加网络的表达能力（非线性）

- sigmoid<br>
- - 饱和效应<br>
误差反向传播过程中导数处于该区域的误差将很难甚至根本无法传递至前层，进而导致整个网络无法训练（导数为􀁹 将无法更新网络参数）。再参数初始化的时候要主要避免直接将参数的输出值域带入这一领域；另外参数初始化过大时，直接引发梯度饱和效应而无法训练。

- ReLU<br>
消除了sigmoid型函数的梯度饱和效应。同时，在实验中还发现相比sigmoid型函数，ReLU函数有助于随机梯度下降方法收敛，收敛速度约快6倍左右。

### 全连接层

在整个网络中起到`分类器`的作业，如果说卷积层、汇合层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的特征表示映射到样本的标记空间的作用。

在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1 × 1 的卷积；而前层是卷积层的全连接层可以化为卷积核为h × w 的全局卷积，h 和w 分别为前层卷积输出结果的高和宽。

### 目标函数

全连接层是将网络特征映射到样本的标记空间做出预测，目标函数的作用则用来衡量该预测值与真实样本标记之间的误差。在当下的卷积神经网络中，交叉熵损失函数和${l_2}$ 损失函数分别是分类问题和回归问题中最为常用的目标函数。

### 重要概念

- 感受野

小卷积核（如3×3）通过多层叠加可取得与大卷积核（如7×7）同等规模的感受野，此外采用小卷积核同时可带来其余两个优势：第一，由于小卷积核需多层叠加，加深了网络深度进而增强了网络容量和复杂度；第二，增强网络容量的同时减少了参数个数。

目前已有不少研究工作为提升模型预测能力通过改造现有卷积操作试图扩大原有卷积核在前层的感受野大小，或使原始感受野不再是矩形区域而是更自由可变的形状，对以上内容感兴趣的读者可参考“扩张卷积操作”和“可变卷积网络”。

- 分布式表示
    
  - “词包”模型(bag of word model)

在计算机视觉中，人们通常将图像局部特征作为一个视觉单词
（visual word），将所有图像的局部特征作为词典（vocabulary），那么一张图像就可以用它的视觉单词来描述，而这些视觉单词又可以通过词典的映射形成一条表示向量（representation vector）。很显然，这样的表示是离散式表示，其表示向量的每个维度可以对应一个明确的视觉模式或概念

神经网络中的“分布式表示” 指“语义概念”到神经元是一个多对多映射，直观来讲，即每个语义概念由许多分布在不同神经元中被激活的模式表示；而每个神经元又可以参与到许多不同语义概念的表示中去。

神经网络响应的区域多呈现“稀疏”特性，即响应区域集中且占原图比例较小。

### 深度特征的层次性

通过反卷积技术对网络可视化，发现浅层核学到的时基本模式，随着网络加深出现高层语义模式。

### Alex Net

五层卷积三层全连接，加入了ReLU、局部规范化、data augmentation、dropout等技巧

- 局部响应规范化（LRN）
局部响应规范化要求对相同空间位置上相邻深度的卷积结果做规范化。假设adi,j 为第d 个通道的卷积核在(i, j) 位置处的输出结果（即响应），随后经过激活函数作用可表示为：

$${b_{i,j}^d=a_{i,j}^d/(k+\alpha\sum_{t=max(0,d-n/2)}^{min(N-1,d+n/2)}(a_{i,j}^t)^2)^\beta}$$

### VGG

VGG-nets中普遍使用了小卷积核以及“保持输入大小”等技巧，为的是在增加网络深度（即网络复杂度）时确保各层输入大小随深度增加而不极具减小。同时，网络卷积层的通道数也是从3 → 64 → 128 → 256 → 512逐渐增加

### Network in Network

它与其他卷积神经网络的最大差异是用多层感知机（多层全连接层和非线性函数的组合）替代了先前卷积网络中简单的线性卷积层。

性卷积层的复杂度有限，利用线性卷积进行层间映射也只能将上层特征或输入“简单”的线性组合形成下层特征。而NIN采用了复杂度更高的多层感知机作为层间映射形式，一方面提供了网络层间映射的一种新可能；另一方面增加了网络卷积层的非线性能力，使得上层特征可有更多复杂性与可能性的映射到下层，这样的想法也被后期出现的残差网络 和Inception等网络模型所借鉴

NIN网络模型的另一个重大突破是摒弃了全连接层作为分类层的传
统，转而改用全局汇合操作（global average pooling）


### 残差网络

深度和宽度时表征网络复杂度的两个核心因素，不够深度比宽度在增加网络复杂性方面更加有效。

随着深度增加，网络训练愈发困难。这主要是因为基于随机梯度下降的网络训练过程中，误差信号的反向传播非常容易引起梯度`弥散`（梯度过小会使得传回的训练误差极其微弱），或者`爆炸`。一些特殊的初始化和Batch norm可以缓解这个问题。

实际情形仍不容乐观。当深度网络收敛时，另外的问题又随之而来：随着继续增加网络的深度，训练数据的训练误差没有降低反而升高。残差网络很好的解决了网络深度带来的训练困难，它的网络性能（完成任务的准确度和精度）远超传统网络模型。

### 高速公路网络


$$
\begin{aligned} 

y=\mathcal F(x,\omega_f) \\ 
y=\mathcal F(x,\omega_f)\circ \mathcal T(x,\omega_t)+x\circ \mathcal C(x,\omega_c)  \\ 
y=\mathcal F(x,\omega_f)\circ \mathcal T(x,\omega_t)+x\circ (1- \mathcal T(x,\omega_t)) 

\end{aligned} 
$$




```mermaid
graph TD;
    A-->B;
    A-->C;
    B-->D;
    C-->D;
```


## 卷积网络压缩

- 前端压缩

包括低秩近似、未加限制的剪枝、参数量化以及二值网络等，其目标在于尽可能地减少模型大小，因而会对原始网络结构造成极大程度的改造

- 后端压缩

不改变原网络结构的压缩技术，主要包括知识蒸馏、紧凑的模型结构设计以及滤波器层面的剪枝等

### 低秩近似

- 结构化矩阵来进行低秩分解

- 直接使用矩阵分解来降低权重矩阵参数

### 剪枝与稀疏约束

- 衡量神经元的重要程度

这也是剪枝算法中最重要的核心步骤。根据剪枝粒度的不同，神经元的定义可以是一个权重接，也可以是整个滤波器。衡量其重要程度的方法也是多种多样，从一些基本的启发式算法，到基于梯度的方案，其计算复杂度与最终的效果也是各有千秋。

- 移除掉一部分不重要的神经元。

根据上一步的衡量结果，剪除掉部分神经元。这里可以根据某个阈值来判断神经元是否可以被剪除，也可以按重要程度排序，剪除掉一定比例的神经元。一般而言，后者比前者更加简便，灵活性也更高。

- 对网络进行微调。

由于剪枝操作会不可避免地影响网络的精度，为防止对
分类性能造成过大的破坏，需要对剪枝后的模型进行微调。对于大规模图
像数据集而言，微调会占用大量的计算资源。因此，对网络微调到什么程度，也是一件需要斟酌的事情。

- 返回第1步，进行下一轮剪枝。


### 参数量化








# Chpater 11 超参数设定和网络训练

## 卷积层参数
小的卷积和更有优势，2\*2，3\*3比较常用，步长建议设置为1

- 增强网络容量和模型复杂度
- 减少卷积参数个数

Padding的作用

- 充分利用输入图像的边缘信息
- 搭配合适的卷积层参数可保持输入/输出同大小  ${p=(f-1)/2}$
- 为了方便硬件计算中划分数据和参数矩阵，通常卷积核设置为2的幂

Pooling Laye

- 一般设定较小的值
- 起到下采样的作用

## 训练技巧

### 训练数据随机打乱

每个epoch进行shuffle，可以略微提升模型在test set上的预测结果

### 学习率设定

- 模型训练开始时不宜过大，否则可以看到目标函数损失急剧上升

- 训练过程中，学习率应随轮数增加而减缓
- - 轮数减缓
- - 指数减缓

- Batch Normalization

内部协变量偏移


### 网络模型优化算法的选择

- SGD

- 基于动量的随机梯度下降

- Nesterov

可以发现，无论是随机梯度下降、基于动量的随机下降法，还是Nesterov型的动量随机下降法，这些优化算法都是为了使梯度更新更加灵活，这对于优化神经网络这种拥有非凸且异常复杂函数空间的学习模型尤为重要。不过，这些方法依然有自身的局限。我们都知道稍小的学习率更加合适网络后期的优化，但这些方法的学习率η 却一直固定不变，并未将学习率的自适应性考虑进去。

- Adagrad

- Adadelta

- RMSProp

- Adam


### 微调神经网络


- 由于网络已在原始数据上收敛，因此应设置较小的学习率在目标数据上微调

- 卷积神经网络浅层拥有更泛化的特征（如边缘、纹理等），深层特征则更抽象对应高层语义。因此，在新数据上微调时泛化特征更新可能或程度较小，高层语义特征更新可能和程度较大，故可根据层深对不同层设置不同学习率：网络深层的学习率可稍大于浅层学习率

- 根据目标任务数据与原始数据相似程度采用不同微调策略：当目标数据较少且目标数据与原始数据非常相似时，可仅微调网络靠近目标函数的后几层；当目标数据充足且相似时，可微调更多网络层，也可全部微调；当目标数据充足但与原始数据差异较大，此时须多调节一些网络层，直至微调全部；当目标数据极少，同时还与原始数据有较大差异时，这种情形比较麻烦，微调成功与否要具体问题具体对待，不过仍可尝试首先微调网络后几层后再微调整个网络模型；

- 此外，针对第三点中提到的“目标数据极少，同时还与原始数据有较大差异”的情况，目前一种有效方式是借助部分原始数据与目标数据协同训练。因预训练模型的浅层网络特征更具泛化性，故可在浅层特征空间选择目标数据的近邻作为原始数据子集。之后，将微调阶段改造为多目标学习任务：一者将目标任务基于原始数据子集，二者将目标任务基于全部目标数据。

# Chapter 12 不平衡样本处理


## 数据层面

### 数据重采样

对深度学习而言下采样并不是直接随机丢弃一部分图像，因为那样做会降低训练数据多样性进而影响模型泛化能力。正确的下采样方式为，在批处理训练时对每批随机抽取的图像严格控制其样本较多类别的图像数量。

### 类别平衡采样

把样本按类别分组，每个类别生成一个样本列表。训练过程中先随机选择􀁒 个或几个类别，然后从各个类别所对应的样本列表中随机选择样本。这样可以保证每个类别参与训练的机会比较均衡。

类别重组法只需原始图像列表即可完成同样的均匀采样任务。其方法步骤如下：首先按照类别顺序对原始样本进行排序，之后计算每个类别的样本数目，并记录样本最多的那个类别的样本数量。之后，根据这个最多样本数对每类样本产生一个随机排列列表，然后用此列表中的随机数对各自类别的样本数求余，得到对应的索引值。接着，根据索引从该类的图像中提取图像，生成该类的图像随机列表。之后，把所有类别的随机列表连在一起随机打乱次序，即可得到最终图像列表，可以发现最终列表中每类样本数目均等。根据此列表训练模型，当训练时列表遍历完毕，则重头再做一遍上述操作即可进行第二轮训练，如此重复下去⋯⋯类别重组法的优点在于，只需原始图像列表，且所有操作均在内存中在线完成，

## 算法层面

### 代价敏感方法

- 代价矩阵

- 代价向量





# Refrence

[解析卷积神经网络—深度学习实践手册](http://lamda.nju.edu.cn/weixs/book/CNN_book.html)
- author Xiu-Shen Wei （魏秀参）

这是一本不错的深度学习入门书，适合初学者。作者是南京大学`lambda lab`的博士生
