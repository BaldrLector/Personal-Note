#

<!-- TOC -->

- [Chapter1 卷积神经网络基础](#chapter1-卷积神经网络基础)
- [Chapter2 卷积网络的基本组件](#chapter2-卷积网络的基本组件)
    - [端到端思想](#端到端思想)
    - [卷积层](#卷积层)
        - [卷积的作用](#卷积的作用)
    - [池化层](#池化层)
    - [激活函数](#激活函数)
    - [全连接层](#全连接层)
    - [目标函数](#目标函数)
- [Chapter3 卷积神经网络经典结构](#chapter3-卷积神经网络经典结构)
    - [重要概念](#重要概念)
    - [深度特征的层次性](#深度特征的层次性)
    - [Alex Net](#alex-net)
    - [VGG](#vgg)
    - [Network in Network](#network-in-network)
    - [残差网络](#残差网络)
    - [高速公路网络](#高速公路网络)
- [Chapter4  卷积网络压缩](#chapter4--卷积网络压缩)
    - [低秩近似](#低秩近似)
    - [剪枝与稀疏约束](#剪枝与稀疏约束)
    - [参数量化](#参数量化)
    - [二值网络](#二值网络)
    - [知识蒸馏](#知识蒸馏)
    - [紧凑的网络结构](#紧凑的网络结构)
- [Chapter5 数据扩充](#chapter5-数据扩充)
    - [简单的数据扩充方式](#简单的数据扩充方式)
    - [特殊的数据扩充方式](#特殊的数据扩充方式)
        - [Fancy PCA](#fancy-pca)
        - [监督式数据扩充](#监督式数据扩充)
- [Chapter6  数据预处理](#chapter6--数据预处理)
- [Chapter7 网络参数初始化](#chapter7-网络参数初始化)
    - [全零初始化](#全零初始化)
    - [随机初始化](#随机初始化)
    - [其他初始化方法](#其他初始化方法)
- [Chapter8 激活函数](#chapter8-激活函数)
    - [Sigmoid](#sigmoid)
    - [tanh](#tanh)
    - [ReLU](#relu)
    - [Leaky relu](#leaky-relu)
    - [参数化ReLU](#参数化relu)
    - [随机化 ReLU](#随机化-relu)
    - [指数化线性单元（ELU）](#指数化线性单元elu)
- [Chapter9 目标函数](#chapter9-目标函数)
    - [分类任务的目标函数](#分类任务的目标函数)
        - [交叉熵](#交叉熵)
        - [合页损失](#合页损失)
        - [坡道损失](#坡道损失)
        - [大间隔交叉熵损失函数](#大间隔交叉熵损失函数)
        - [中心损失函数](#中心损失函数)
    - [回归任务的目标函数](#回归任务的目标函数)
        - [${l_1}$损失](#l_1损失)
        - [${l_2}$损失](#l_2损失)
        - [Tukey`s biweight损失函数](#tukeys-biweight损失函数)
        - [Kullback-Leibler散度，KL loss](#kullback-leibler散度kl-loss)
- [Chapter10 网络正则化](#chapter10-网络正则化)
    - [${L_2}$正则](#l_2正则)
    - [${L_1}$正则](#l_1正则)
    - [最大范数约束](#最大范数约束)
    - [随机失活（Dropout）](#随机失活dropout)
    - [验证集的使用](#验证集的使用)
- [Chpater 11 超参数设定和网络训练](#chpater-11-超参数设定和网络训练)
    - [卷积层参数](#卷积层参数)
    - [训练技巧](#训练技巧)
        - [训练数据随机打乱](#训练数据随机打乱)
        - [学习率设定](#学习率设定)
        - [网络模型优化算法的选择](#网络模型优化算法的选择)
        - [微调神经网络](#微调神经网络)
- [Chapter 12 不平衡样本处理](#chapter-12-不平衡样本处理)
    - [数据层面](#数据层面)
        - [数据重采样](#数据重采样)
        - [类别平衡采样](#类别平衡采样)
    - [算法层面](#算法层面)
        - [代价敏感方法](#代价敏感方法)
- [Chapter13 模型集成方法](#chapter13-模型集成方法)
    - [数据层面的集成方法](#数据层面的集成方法)
        - [测试阶段数据扩充](#测试阶段数据扩充)
        - [“简易集成”法](#简易集成法)
    - [模型层面的集成方法](#模型层面的集成方法)
        - [单模型集成](#单模型集成)
            - [多层特征融合](#多层特征融合)
            - [网络“快照”集成法](#网络快照集成法)
        - [多模型集成](#多模型集成)
            - [多模型生成策略](#多模型生成策略)
            - [多模型集成方法](#多模型集成方法)
- [Refrence](#refrence)

<!-- /TOC -->

# Chapter1 卷积神经网络基础

- 前馈

- 后馈

- BP

# Chapter2 卷积网络的基本组件

## 端到端思想

## 卷积层
### 卷积的作用

卷积是一种局部操作，通过一定大小的卷积核作用于局部图像区域获得图像的局部信息。试想，若原图像素`(x, y)`处可能存在物体边缘，则其四周`(x−1, y)`，`(x+1, y)`，`(x, y − 1)`，`(x, y + 1)`处像素值应与`(x, y)`处有显著差异。此时，如作用以整体边缘滤波器${K_e}$，可消除四周像素值差异小的图像区域而保留显著差异区域，以此可检测出物体边缘信息。同理，类似${K_h}$ 和${K_V^3}$的横向、纵向边缘滤波器可分别保留横向、纵向的边缘信息

事实上，卷积网络中的卷积核参数是通过网络训练学出的，除了可以学到类似的横向、纵向边缘滤波器，还可以学到任意角度的边缘滤波器。当然，不仅如此，检测颜色、形状、纹理等等众多基本模式的滤波器（卷积核）都可以包含在一个足够复杂的深层卷积神经网络中。通过“组合”这些滤波器（卷积核）以及随着网络后续操作的进行，基本而一般的模式会逐渐被抽象为具有高层语义的“概念”表示，并以此对应到具体的样本类别。颇有“盲人摸象”后，将各自结果集大成之意。

## 池化层
Max、Average、Stochastic pooling<br>
`池化`是一种常见的降采样方法，也可以看成一个用p-范数作为非线性映射的`卷积`<br>
研究者普遍认为池化层有以下作业：
- 特征不变性<br>
汇合操作使模型更关注是否存在某些特征而不是特征具体的位置。可看作是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。
- 特征降维<br>
由于汇合操作的降采样作用，汇合结果中的一个元素对应于原
输入数据的一个子区域，因此汇合相当于在空间范围内做了
维度约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。
- 一定程度防止过拟合

## 激活函数

目的：增加网络的表达能力（非线性）

- sigmoid<br>
- - 饱和效应<br>
误差反向传播过程中导数处于该区域的误差将很难甚至根本无法传递至前层，进而导致整个网络无法训练（导数为􀁹 将无法更新网络参数）。再参数初始化的时候要主要避免直接将参数的输出值域带入这一领域；另外参数初始化过大时，直接引发梯度饱和效应而无法训练。

- ReLU<br>
消除了sigmoid型函数的梯度饱和效应。同时，在实验中还发现相比sigmoid型函数，ReLU函数有助于随机梯度下降方法收敛，收敛速度约快6倍左右。

## 全连接层

在整个网络中起到`分类器`的作业，如果说卷积层、汇合层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的特征表示映射到样本的标记空间的作用。

在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1 × 1 的卷积；而前层是卷积层的全连接层可以化为卷积核为h × w 的全局卷积，h 和w 分别为前层卷积输出结果的高和宽。

## 目标函数

全连接层是将网络特征映射到样本的标记空间做出预测，目标函数的作用则用来衡量该预测值与真实样本标记之间的误差。在当下的卷积神经网络中，交叉熵损失函数和${l_2}$ 损失函数分别是分类问题和回归问题中最为常用的目标函数。


# Chapter3 卷积神经网络经典结构

## 重要概念

- 感受野

小卷积核（如3×3）通过多层叠加可取得与大卷积核（如7×7）同等规模的感受野，此外采用小卷积核同时可带来其余两个优势：第一，由于小卷积核需多层叠加，加深了网络深度进而增强了网络容量和复杂度；第二，增强网络容量的同时减少了参数个数。

目前已有不少研究工作为提升模型预测能力通过改造现有卷积操作试图扩大原有卷积核在前层的感受野大小，或使原始感受野不再是矩形区域而是更自由可变的形状，对以上内容感兴趣的读者可参考“扩张卷积操作”和“可变卷积网络”。

- 分布式表示
    
  - “词包”模型(bag of word model)

在计算机视觉中，人们通常将图像局部特征作为一个视觉单词
（visual word），将所有图像的局部特征作为词典（vocabulary），那么一张图像就可以用它的视觉单词来描述，而这些视觉单词又可以通过词典的映射形成一条表示向量（representation vector）。很显然，这样的表示是离散式表示，其表示向量的每个维度可以对应一个明确的视觉模式或概念

神经网络中的“分布式表示” 指“语义概念”到神经元是一个多对多映射，直观来讲，即每个语义概念由许多分布在不同神经元中被激活的模式表示；而每个神经元又可以参与到许多不同语义概念的表示中去。

神经网络响应的区域多呈现“稀疏”特性，即响应区域集中且占原图比例较小。

## 深度特征的层次性

通过反卷积技术对网络可视化，发现浅层核学到的时基本模式，随着网络加深出现高层语义模式。

## Alex Net

五层卷积三层全连接，加入了ReLU、局部规范化、data augmentation、dropout等技巧

- 局部响应规范化（LRN）
局部响应规范化要求对相同空间位置上相邻深度的卷积结果做规范化。假设adi,j 为第d 个通道的卷积核在(i, j) 位置处的输出结果（即响应），随后经过激活函数作用可表示为：

$${b_{i,j}^d=a_{i,j}^d/(k+\alpha\sum_{t=max(0,d-n/2)}^{min(N-1,d+n/2)}(a_{i,j}^t)^2)^\beta}$$

## VGG

VGG-nets中普遍使用了小卷积核以及“保持输入大小”等技巧，为的是在增加网络深度（即网络复杂度）时确保各层输入大小随深度增加而不极具减小。同时，网络卷积层的通道数也是从3 → 64 → 128 → 256 → 512逐渐增加

## Network in Network

它与其他卷积神经网络的最大差异是用多层感知机（多层全连接层和非线性函数的组合）替代了先前卷积网络中简单的线性卷积层。

性卷积层的复杂度有限，利用线性卷积进行层间映射也只能将上层特征或输入“简单”的线性组合形成下层特征。而NIN采用了复杂度更高的多层感知机作为层间映射形式，一方面提供了网络层间映射的一种新可能；另一方面增加了网络卷积层的非线性能力，使得上层特征可有更多复杂性与可能性的映射到下层，这样的想法也被后期出现的残差网络 和Inception等网络模型所借鉴

NIN网络模型的另一个重大突破是摒弃了全连接层作为分类层的传
统，转而改用全局汇合操作（global average pooling）


## 残差网络

深度和宽度时表征网络复杂度的两个核心因素，不够深度比宽度在增加网络复杂性方面更加有效。

随着深度增加，网络训练愈发困难。这主要是因为基于随机梯度下降的网络训练过程中，误差信号的反向传播非常容易引起梯度`弥散`（梯度过小会使得传回的训练误差极其微弱），或者`爆炸`。一些特殊的初始化和Batch norm可以缓解这个问题。

实际情形仍不容乐观。当深度网络收敛时，另外的问题又随之而来：随着继续增加网络的深度，训练数据的训练误差没有降低反而升高。残差网络很好的解决了网络深度带来的训练困难，它的网络性能（完成任务的准确度和精度）远超传统网络模型。

## 高速公路网络


$$
\begin{aligned} 
y=\mathcal F(x,\omega_f) \\ 
y=\mathcal F(x,\omega_f)\circ \mathcal T(x,\omega_t)+x\circ \mathcal C(x,\omega_c)  \\ 
y=\mathcal F(x,\omega_f)\circ \mathcal T(x,\omega_t)+x\circ (1- \mathcal T(x,\omega_t)) 
\end{aligned} 
$$




# Chapter4  卷积网络压缩

- 前端压缩

包括低秩近似、未加限制的剪枝、参数量化以及二值网络等，其目标在于尽可能地减少模型大小，因而会对原始网络结构造成极大程度的改造

- 后端压缩

不改变原网络结构的压缩技术，主要包括知识蒸馏、紧凑的模型结构设计以及滤波器层面的剪枝等

## 低秩近似

- 结构化矩阵来进行低秩分解

- 直接使用矩阵分解来降低权重矩阵参数

## 剪枝与稀疏约束

- 衡量神经元的重要程度

这也是剪枝算法中最重要的核心步骤。根据剪枝粒度的不同，神经元的定义可以是一个权重接，也可以是整个滤波器。衡量其重要程度的方法也是多种多样，从一些基本的启发式算法，到基于梯度的方案，其计算复杂度与最终的效果也是各有千秋。

- 移除掉一部分不重要的神经元。

根据上一步的衡量结果，剪除掉部分神经元。这里可以根据某个阈值来判断神经元是否可以被剪除，也可以按重要程度排序，剪除掉一定比例的神经元。一般而言，后者比前者更加简便，灵活性也更高。

- 对网络进行微调。

由于剪枝操作会不可避免地影响网络的精度，为防止对
分类性能造成过大的破坏，需要对剪枝后的模型进行微调。对于大规模图
像数据集而言，微调会占用大量的计算资源。因此，对网络微调到什么程度，也是一件需要斟酌的事情。

- 返回第1步，进行下一轮剪枝。


## 参数量化



## 二值网络

量化方法的一种极端情况：所有参数的取值只能是+/-1

两个基本问题：

- 如何对权重进行二值化？

权重二值化，通常有两种选择：一是直接根据权重的正负进行二值化${x^b=sign(x)}$；一是进行随机的二值化，即对每一个权重，以一定的概率取+1。在实际过程中，随机数的产生会非常耗时，因此，第一种策略更加实用。

- 如何计算二值权重的梯度？

由于二值权重的梯度为0，无法进行参数更新。为了解决这个问题，需要对符号函数进行放松，即用${Htanh(x)=max(-1,min(1,x))}$来代替${sign(x)}$。当x 在区间[−1,1]时，存在梯度值1，否则梯度为0。

模型的训练过程中，存在着两种类型的权重:
- 原始的单精度权重
- 由该单精度权重得到的二值权重

## 知识蒸馏

迁移学习的一种，其最终目的是将一个庞大而复杂的模型所学到的知识，通过一定的技术手段迁移到精简的小模型上，使得小模型能够获得与大模型相近的性能。


## 紧凑的网络结构

- 挤压

为追求模型容量于参数平衡，用1*1的卷积核对输入特征降维，1 × 1 的卷积可综合多个通道的信息，得到更加紧凑的输入特征，从而保证了模型的泛化性

- 扩张

用1\*1代替3\*3的卷积核，为了使得不同卷积核的输出能够拼接出完整输出，需要padding


# Chapter5 数据扩充

数据扩充是深度模型训练前的必须一步，此操作可扩充训练数据集，增强数据多样性，防止模型过拟合

## 简单的数据扩充方式

- 水平翻转

- 随机扣取

- 尺度变换和旋转

- 尺度变换、旋转等，从而增加卷积神经网络对物体尺度和方向上的鲁棒性

- 色彩抖动

## 特殊的数据扩充方式

### Fancy PCA

### 监督式数据扩充






# Chapter6  数据预处理

卷积神经网络中的数据预处理通常是计算训练集图像像素均值，之后在处理训练集、验证集和测试集图像时需要分别减去该均值

减均值操作的原理是，我们默认自然图像是一类平稳的数据分布（即数据每一个维度的统计都服从相同分布），此时，在每个样本上减去数据的统计平均值（逐样本计算）可以移除共同部分，凸显个体差异


# Chapter7 网络参数初始化

## 全零初始化

## 随机初始化

- Xavier

- He initialization

## 其他初始化方法

利用预训练模型


# Chapter8 激活函数

## Sigmoid

压缩输出到[0,1]，两端大于+-5的部分梯度接近0

## tanh

有饱和效应，zero-centred

## ReLU

- 相对前两者的指数函数计算更为简单，有助于随机梯度下降方法收敛，收敛速度约快6倍
- 一旦变为负值将再无法影响网络训练

## Leaky relu

有超参数，在实际使用中的性能并不十分稳定


## 参数化ReLU

- 与第一层卷积层搭配的参数化ReLU的α 取值远大于ReLU中的0。这表明网络较浅层所需非线性较弱。同时，我们知道浅层网络特征一般多为表示“边缘”、“纹理”等特性的泛化特征。这一观察说明对于此类特征正负响应均很重要；这也解释了固定α 取值的α （α = 0）和Leaky relu相比参数化ReLu性能较差的原因。

- 请注意独享参数设定下学到的α 取值呈现由浅层到深层依次递减的趋势，说明实际上网络所需的非线性能力随网络深度增加而递增。

## 随机化 ReLU



## 指数化线性单元（ELU）

$$ELU(x)=\begin{cases} 
		x, & if x \ge 0\\ 
		\lambda (exp(x)-1), & if x<0\ 
	\end{cases}$$

ELU具备ReLU函数的优点，同时也解决了ReLU函数自身的“死区”问题。不过，ELU函数中的指数操作稍稍增大了计算量。实际使用中，超参数λ 一般设置为1。

# Chapter9 目标函数

## 分类任务的目标函数

### 交叉熵

### 合页损失

### 坡道损失

### 大间隔交叉熵损失函数

### 中心损失函数

## 回归任务的目标函数

### ${l_1}$损失

### ${l_2}$损失

### Tukey`s biweight损失函数

### Kullback-Leibler散度，KL loss


# Chapter10 网络正则化

## ${L_2}$正则

## ${L_1}$正则

## 最大范数约束

## 随机失活（Dropout）

随机失活的提出正是一定程度上缓解了神经元之间复杂的协同适应，降低了神经元间依赖，避免了网络过拟合的发生

## 验证集的使用




# Chpater 11 超参数设定和网络训练

## 卷积层参数
小的卷积和更有优势，2\*2，3\*3比较常用，步长建议设置为1

- 增强网络容量和模型复杂度
- 减少卷积参数个数

Padding的作用

- 充分利用输入图像的边缘信息
- 搭配合适的卷积层参数可保持输入/输出同大小  ${p=(f-1)/2}$
- 为了方便硬件计算中划分数据和参数矩阵，通常卷积核设置为2的幂

Pooling Laye

- 一般设定较小的值
- 起到下采样的作用

## 训练技巧

### 训练数据随机打乱

每个epoch进行shuffle，可以略微提升模型在test set上的预测结果

### 学习率设定

- 模型训练开始时不宜过大，否则可以看到目标函数损失急剧上升

- 训练过程中，学习率应随轮数增加而减缓
- - 轮数减缓
- - 指数减缓

- Batch Normalization

内部协变量偏移


### 网络模型优化算法的选择

- SGD

$${w_t \gets w_{t-1}- \eta g}$$

- 基于动量的随机梯度下降

$${v_t \gets \mu v_{t-1} - \mu g}$$
$${w_t \gets w_{t-1} +v_{t}}$$

- Nesterov

$${w_{ahead} \gets w_{t-1}+\mu v_{t-1}}$$
$${v_t \gets \mu v_{t-1} - \eta \bigtriangledown_{w_{ahead}}}$$

$${w_{t} \gets w_{t-1} + v_t}$$

可以发现，无论是随机梯度下降、基于动量的随机下降法，还是Nesterov型的动量随机下降法，这些优化算法都是为了使梯度更新更加灵活，这对于优化神经网络这种拥有非凸且异常复杂函数空间的学习模型尤为重要。不过，这些方法依然有自身的局限。我们都知道稍小的学习率更加合适网络后期的优化，但这些方法的学习率η 却一直固定不变，并未将学习率的自适应性考虑进去。

- Adagrad

$${w_t=\frac{\eta_{global}}{\sqrt{\sum_{t'=1}^tg_{t'}^2 +\epsilon}}}$$

依赖于一个全局学习率

- Adadelta

Adadelta是对Adagrad的扩展，通过引入衰减因子ρ 消除Adagrad法对全局学习率的依赖

$${r_t \gets \rho \cdot r_{t-1} +(1-\rho)\cdot g^2}$$
$${\eta_t \gets \frac{\sqrt{s_{t-1}+\epsilon}}{\sqrt{r_t}+\epsilon}}$$
$${s_t \gets \rho \cdot s_{t-1} + (1-\rho)\cdot (\eta_t \cdot g)^2}$$

- RMSProp

可视作Adadelta的一个特例，即依然使用全局学习率替换
掉Adadelta中的${s_t}$

$${r_t \gets \rho \cdot r_{t-1} + (1-\rho)\cdot g^2}$$
$${\eta_t \gets \frac{\eta_{global}}{\sqrt{r_t+\epsilon}}}$$

- Adam

$${m_t \gets \beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t}$$
$${v_t\gets \beta_2 \cdot v_{t-1}+(1-\beta_2)\cdot g_t^2}$$
$${\hat m \gets \frac{m_t}{1-\beta_1^t}}$$
$${\hat v \gets \frac{v_t}{1-\beta_2^t}}$$
$${w_t\gets w_{t-1}-\eta\cdot\frac{\hat m_t}{\sqrt{\hat v_t +\epsilon}}}$$


### 微调神经网络


- 由于网络已在原始数据上收敛，因此应设置较小的学习率在目标数据上微调

- 卷积神经网络浅层拥有更泛化的特征（如边缘、纹理等），深层特征则更抽象对应高层语义。因此，在新数据上微调时泛化特征更新可能或程度较小，高层语义特征更新可能和程度较大，故可根据层深对不同层设置不同学习率：网络深层的学习率可稍大于浅层学习率

- 根据目标任务数据与原始数据相似程度采用不同微调策略：当目标数据较少且目标数据与原始数据非常相似时，可仅微调网络靠近目标函数的后几层；当目标数据充足且相似时，可微调更多网络层，也可全部微调；当目标数据充足但与原始数据差异较大，此时须多调节一些网络层，直至微调全部；当目标数据极少，同时还与原始数据有较大差异时，这种情形比较麻烦，微调成功与否要具体问题具体对待，不过仍可尝试首先微调网络后几层后再微调整个网络模型；

- 此外，针对第三点中提到的“目标数据极少，同时还与原始数据有较大差异”的情况，目前一种有效方式是借助部分原始数据与目标数据协同训练。因预训练模型的浅层网络特征更具泛化性，故可在浅层特征空间选择目标数据的近邻作为原始数据子集。之后，将微调阶段改造为多目标学习任务：一者将目标任务基于原始数据子集，二者将目标任务基于全部目标数据。

# Chapter 12 不平衡样本处理


## 数据层面

### 数据重采样

对深度学习而言下采样并不是直接随机丢弃一部分图像，因为那样做会降低训练数据多样性进而影响模型泛化能力。正确的下采样方式为，在批处理训练时对每批随机抽取的图像严格控制其样本较多类别的图像数量。

### 类别平衡采样

把样本按类别分组，每个类别生成一个样本列表。训练过程中先随机选择1个或几个类别，然后从各个类别所对应的样本列表中随机选择样本。这样可以保证每个类别参与训练的机会比较均衡。

类别重组法只需原始图像列表即可完成同样的均匀采样任务。其方法步骤如下：首先按照类别顺序对原始样本进行排序，之后计算每个类别的样本数目，并记录样本最多的那个类别的样本数量。之后，根据这个最多样本数对每类样本产生一个随机排列列表，然后用此列表中的随机数对各自类别的样本数求余，得到对应的索引值。接着，根据索引从该类的图像中提取图像，生成该类的图像随机列表。之后，把所有类别的随机列表连在一起随机打乱次序，即可得到最终图像列表，可以发现最终列表中每类样本数目均等。根据此列表训练模型，当训练时列表遍历完毕，则重头再做一遍上述操作即可进行第二轮训练，如此重复下去⋯⋯类别重组法的优点在于，只需原始图像列表，且所有操作均在内存中在线完成

## 算法层面

### 代价敏感方法

- 代价矩阵

- 代价向量

# Chapter13 模型集成方法

## 数据层面的集成方法

### 测试阶段数据扩充

数据扩充策略在模型测试阶段同样适用

### “简易集成”法

对于样本较多的类采取降采样，每次采样数依照样本数目最少的类别而定，这样每类取到的样本数可保持均等。采样结束后，针对每次采样得到的子数据集训练模型，如此采样、训练反复进行多次。最后对测试数据的预测则依据训练得到若干个模型的结果取平均或投票获得。


## 模型层面的集成方法

### 单模型集成

#### 多层特征融合

多层特征融合操作时可直接将不同层网络特征级联。而对于特征融合应选取哪些网络层，一个实践经验是：最好使用靠近目标函数的几层卷积特征，因为愈深层特征包含的高层语义性愈强、分辨能力也愈强；相反，网络较浅层的特征较普适，用于特征融合很可能起不到作用有时甚至会起到相反作用。

#### 网络“快照”集成法

$${\eta(t)=\frac{\eta_0}{2}\left(cos\left(\frac{\pi mod(t-1,[T/M])}{[T/M]}+1\right)\right)}$$

### 多模型集成

#### 多模型生成策略

- 同一模型不同初始化

由于神经网络训练机制基于随机梯度下降，故不同的网络模型参数初始化会导致不同的网络训练结果。在实际使用中，特别是针对小样本学习的场景，首先对同一模型进行不同初始化，之后将得到的网络模型进行结果集成会大幅缓解其随机性，提升最终任务的预测结果。

- 同一模型不同训练轮数

若网络超参数设置得当，深度模型随着网络训练的进行会逐步趋于收敛，但不同训练轮数的结果仍有不同，无法确定到底哪一轮训练得到的模型最适用于测试数据。针对上述问题，一种简单的解决方式是将最后几轮训练模型结果做集成，这样一方面可降低随机误差，另一方面也避免了训练轮数过多带来的过拟合风险。这样的操作被称为“轮数集成”

- 不同目标函数

在预测阶段，既可以直接对不同模型预测结果做“置信度级别”的平均或投票，也可以做“特征级别”的模型集成：将不同网络得到的深度特征抽出后级联作为最终特征，之后离线训练浅层分类器（如支持向量机）完成预测任务。

- 不同网络结构

操作时可在如VGG网络、深度残差网络等不同网络架构的网络上训练模型，最后将不同架构网络得到的结果做以集成

#### 多模型集成方法

- 直接平均法

$${Final score = \frac{\sum_{i=0}^N{s_i}}{N}}$$

- 加权平均法

$${Final score = \frac{\sum_{i=0}^N{w_is_i}}{N}}$$
$${w_i \ge 0 , \sum_{i=0}^N{w_i}=1}$$

- 投票法

- 堆叠法


```mermaid
graph LR

train((train)) --good bias--> validation((validation))
test --bad bias-->validation

validation --good variance-->test((test))
test -- change loss function --> train

```

# Refrence

[解析卷积神经网络—深度学习实践手册](http://lamda.nju.edu.cn/weixs/book/CNN_book.html)
- author Xiu-Shen Wei （魏秀参）

这是一本不错的深度学习入门书，适合初学者。作者是南京大学`lambda lab`的博士生
