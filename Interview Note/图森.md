# 一面

- Batch Norm和Dropout在训练和测试阶段的区别（Dropout的rescale）
- 混淆矩阵的原理和应用，为什么要用F1调和准确率和召回率，单独使用准确率和召回率有什么问题
- 正则和dropout的本质是什么（面试官：正则本质是奥卡姆梯度原理，dropout本质是模型融合）
- 困难样本的训练：如果样本有遮蔽怎么做识别，比如识别猫狗，猫的样本里面有一张前面挡了一堵墙，这个样本要不要特殊处理（我回答：如果是这样的样本，在训练的模型里面，猫狗两类的评分是差不多的，都是0.5，然后挑出这些样本，给他们loss更高的权重）
